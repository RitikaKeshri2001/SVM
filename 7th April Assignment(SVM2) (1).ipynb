{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fcae0b-969f-43a4-a4d6-5038be6bb9d6",
   "metadata": {},
   "source": [
    "#### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "Kernel functions are used to transform the original feature space into a higher dimensional feature space, where a linear hyperplane can better separate the data points.\n",
    "\n",
    "Polynomial functions are one type of kernel function used in SVMs. Specifically, a polynomial kernel is a type of kernel function that uses a polynomial of a certain degree to map the original features to a higher dimensional space. The degree of the polynomial determines the degree of the mapping, and higher degrees result in a larger increase in the dimensionality of the feature space.\n",
    "\n",
    "Polynomial functions and kernel functions are related in machine learning algorithms because polynomial kernel is a type of kernel function that can be used to transform the input data into a higher-dimensional feature space where linear models can be applied to non-linear problems. Polynomial kernel represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables. Polynomial kernel is defined as:\n",
    "\n",
    "K(x,y)=(x<sup>T</sup>y+c)<sup>d</sup>\n",
    "\n",
    "where x and y are vectors in the input space, c â‰¥ 0 is a free parameter trading off the influence of higher-order versus lower-order terms in the polynomial, and d is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189c02c-e4cb-41db-9528-0275423dec0f",
   "metadata": {},
   "source": [
    "#### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "To implement an SVM with a polynomial kernel in python using Scikit-learn as:\n",
    "1. import the necessary libraries such as svm, datasets from sklearn\n",
    "2. Load the dataset and split the dataset into train and test data using train_test_split.\n",
    "3. Create an instance of SVC() class and specify kernel parameter as poly.\n",
    "4. Train the model using fit method.\n",
    "5. Make prediction on the trained model using predict method on test data.\n",
    "6. Evaluate the performance of the model using accuracy_score, confusion matrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8021865c-6e71-40b4-abb5-5efc1109b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9777777777777777\n",
      "confusion metrix: \n",
      " [[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# load the dataset\n",
    "df = load_iris()\n",
    "X = df.data    # features\n",
    "y = df.target  # target\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# create the instance of svc\n",
    "svc = SVC(kernel='poly')\n",
    "\n",
    "# train the data using fit method\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# test the data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"confusion metrix: \\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8f0be-b31b-44e6-848b-87923ceb8a11",
   "metadata": {},
   "source": [
    "#### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Epsilon is a parameter that controls the width of the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. In other words, epsilon defines a margin of tolerance where errors are ignored. The larger epsilon is, the larger errors we admit in our solution, and the fewer support vectors we will have. Conversely, the smaller epsilon is, the more strict we are with errors, and the more support vectors we will have.\n",
    "\n",
    "Therefore, increasing the value of epsilon will decrease the number of support vectors in SVR, and vice versa. However, choosing an appropriate value of epsilon is a trade-off between model complexity and generalization ability. A too large epsilon may result in underfitting, while a too small epsilon may result in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751cb84-0378-4a06-8936-7dc015953cb3",
   "metadata": {},
   "source": [
    "#### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Kernel function:</br> \n",
    "This parameter specifies the type of function that is used to map the input data into a higher-dimensional feature space where linear models can be applied to non-linear problems. There are different types of kernel functions, such as linear, polynomial, radial basis function (RBF), sigmoid, etc. Each kernel function has its own advantages and disadvantages, depending on the nature and complexity of the data. For example, linear kernel is simple and fast, but it may not capture complex patterns in the data. Polynomial kernel can capture feature interactions, but it may suffer from numerical instability. RBF kernel can handle non-linear and flexible data, but it may be sensitive to outliers and noise. The choice of kernel function depends on the data and the problem at hand.\n",
    "\n",
    "C parameter:</br> \n",
    "This parameter controls the regularization strength of the SVR model. It is inversely proportional to the norm of the function, which means a larger C means less regularization and a smaller C means more regularization. Regularization is a technique that prevents overfitting by adding a penalty term to the cost function that shrinks the coefficients towards zero. The optimal value of C depends on the trade-off between bias and variance. Bias is the error due to incorrect assumptions or oversimplification of the model. Variance is the error due to sensitivity or instability of the model to small changes in the data. A too large C may result in high variance and low bias, which means overfitting. A too small C may result in low variance and high bias, which means underfitting. We might want to increase C if we have a lot of data and low noise, or decrease C if we have a small amount of data and high noise.\n",
    "\n",
    "Epsilon parameter:</br> \n",
    "Epsilon is a parameter that controls the width of the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. In other words, epsilon defines a margin of tolerance where errors are ignored. The larger epsilon is, the larger errors we admit in our solution, and the fewer support vectors we will have. Conversely, the smaller epsilon is, the more strict we are with errors, and the more support vectors we will have. Therefore, increasing the value of epsilon will decrease the number of support vectors in SVR, and vice versa. However, choosing an appropriate value of epsilon is a trade-off between model complexity and generalization ability. A too large epsilon may result in underfitting, while a too small epsilon may result in overfitting.\n",
    "\n",
    "Gamma parameter:</br> \n",
    "This parameter is only relevant for kernel functions that involve distance or similarity measures, such as RBF, polynomial, or sigmoid kernels. It controls how much influence a single training example has on the decision boundary. It is directly proportional to the width of the kernel function, which means a larger gamma means a narrower kernel and a smaller gamma means a wider kernel. A narrower kernel means that each training example has a small region of influence around it, which results in a more complex and wiggly decision boundary. A wider kernel means that each training example has a large region of influence around it, which results in a smoother and simpler decision boundary. The optimal value of gamma depends on the trade-off between bias and variance as well. A too large gamma may result in high variance and low bias, which means overfitting. A too small gamma may result in low variance and high bias, which means underfitting. We might want to increase gamma if we have a lot of features and low noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648628c-c84e-4b55-8712-2dc762b0445e",
   "metadata": {},
   "source": [
    "#### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets.\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance.\n",
    "- Train the tuned classifier on the entire dataset.\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c152051-2e89-4105-aa60-1034e4b3f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Confusion Matrix: \n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "df = load_iris()\n",
    "X = df.data    # features\n",
    "y = df.target  # target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess the data using StandardScler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_train, y_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using any metric \n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ab83b1-ae37-412f-8226-6b9095cd095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C':[0.1, 1, 10, 100, 1000],\n",
    "    'gamma':[1, 0.1, 0.01, 0.0001],\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svc_tuning = GridSearchCV(SVC(), param_grid=param_grid, refit=True, cv = 5)\n",
    "svc_tuning.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = svc_tuning.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d28516-a79e-46e9-82fd-00d1024a8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier to a file for future use\n",
    "import pickle\n",
    "\n",
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))\n",
    "pickle.dump(svc_tuning, open('svc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1811f-d5c7-4274-9b60-4e68b4e26940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
